{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLING REQUIRED LIBRARIES\n"
      ],
      "metadata": {
        "id": "k-2NaW4xtXEQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYacWc6atNpX"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install langchain langchain_community sentence-transformers chromadb\n",
        "\n",
        "!pip uninstall fitz -y\n",
        "!pip install pymupdf\n",
        "!pip install re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DIVIDING DATA INTO CHUNKS"
      ],
      "metadata": {
        "id": "EuOZrSn9tWYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pymupdf\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Paths\n",
        "CHROMA_PATH = \"\" # Define path to where to save data after divinding in chunks\n",
        "PDF_PATH = \"\"  # Define path to your data\n",
        "\n",
        "# Section Header Patterns\n",
        "SECTION_PATTERN = r\"(Champions Trophy Overview|ICC Champions Trophy 2025 ‚Äì Schedule|Champions Trophy Winners List|All-Time Batting Stats|All-Time Bowling Stats|Most Runs in Champions Trophy|Most Wickets in Champions Trophy|Highest Team Totals|Champions Trophy 2025 Squads)\"\n",
        "\n",
        "def main():\n",
        "    generate_data_store()\n",
        "\n",
        "def generate_data_store():\n",
        "    documents = load_pdf(PDF_PATH)\n",
        "    chunks = split_text(documents)\n",
        "    save_to_chroma(chunks)\n",
        "\n",
        "def load_pdf(file_path: str):\n",
        "    documents = []\n",
        "    with pymupdf.open(file_path) as pdf_doc:\n",
        "        text_content = \"\"\n",
        "        for page in pdf_doc:\n",
        "            text_content += page.get_text(\"text\") + \"\\n\"\n",
        "\n",
        "        # Split based on section headers\n",
        "        sections = re.split(SECTION_PATTERN, text_content)\n",
        "        headers = re.findall(SECTION_PATTERN, text_content)\n",
        "\n",
        "        for i, section in enumerate(sections):\n",
        "            cleaned_section = section.strip()\n",
        "            if cleaned_section:\n",
        "                section_title = headers[i-1] if i > 0 and i <= len(headers) else \"Unknown Section\"\n",
        "                doc = Document(\n",
        "                    page_content=cleaned_section,\n",
        "                    metadata={\"filename\": os.path.basename(file_path), \"section\": section_title}\n",
        "                )\n",
        "                documents.append(doc)\n",
        "\n",
        "    return documents\n",
        "\n",
        "def split_text(documents: list[Document]):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1500,  # Increased chunk size for better retrieval\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} sections into {len(chunks)} chunks.\")\n",
        "    return chunks\n",
        "\n",
        "def save_to_chroma(chunks: list[Document]):\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
        "    db = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)\n",
        "    db.persist()\n",
        "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "TurzEbAAtTXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUNNING INFERENCE ON DATA USING MISTRAL MODEL"
      ],
      "metadata": {
        "id": "xd1iqipCtqHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define path\n",
        "CHROMA_PATH = \"\"\n",
        "\n",
        "# Hugging Face API Key (Replace with your actual key)\n",
        "HF_API_KEY = \"\"\n",
        "\n",
        "client = InferenceClient(model=\"mistralai/Mistral-Small-24B-Instruct-2501\", token=HF_API_KEY)\n",
        "\n",
        "# Define prompt template\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context. If the answer is not present in the context, answer based on your own knowledge.\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question: {question}\n",
        "\"\"\"\n",
        "\n",
        "def query_database(query_text):\n",
        "    # Use BAAI/bge-small-en embeddings (Local, No API Required)\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
        "\n",
        "    # Search for relevant chunks\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=10)\n",
        "\n",
        "    if len(results) == 0 or results[0][1] < 0.7:\n",
        "        context_text = \"No relevant context found.\"\n",
        "    else:\n",
        "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "\n",
        "    # Prepare the request for the model\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\"max_tokens\": 500}\n",
        "    }\n",
        "\n",
        "    response = client.post(json=payload)\n",
        "\n",
        "    # Extract response safely\n",
        "    try:\n",
        "        response_data = json.loads(response)\n",
        "        response_text = response_data[0]['generated_text'] if isinstance(response_data, list) else response_data['generated_text']\n",
        "    except (json.JSONDecodeError, KeyError, IndexError):\n",
        "        response_text = \"‚ùå Error generating response.\"\n",
        "\n",
        "    sources = [doc.metadata.get(\"source\", \"Unknown Source\") for doc, _score in results]\n",
        "    formatted_response = f\"Response: {response_text}\\n\\n\\nSources: {sources}\"\n",
        "    print(formatted_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = input(\"üîé Enter your query: \")\n",
        "    query_database(query)"
      ],
      "metadata": {
        "id": "W7qXY9IBtxxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUNNING INFERENCE ON DATA USING GEMMA MODEL\n",
        "\n"
      ],
      "metadata": {
        "id": "8F0ou1lft7wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Define path\n",
        "CHROMA_PATH = \"\"\n",
        "\n",
        "# Hugging Face API Key (Replace with your actual key)\n",
        "HF_API_KEY = \"\"\n",
        "\n",
        "# Use supported chat model\n",
        "HF_MODEL = \"google/gemma-2-9b-it\"\n",
        "\n",
        "client = InferenceClient(provider=\"together\", api_key=HF_API_KEY)\n",
        "\n",
        "# Define prompt template\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert cricket analyst. Answer the user's question **only** using the given context.\n",
        "If the answer is **not in the context**, say: \"The answer is not available in the provided information.\"\n",
        "\n",
        "---\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "def query_database(query_text, top_k=10, min_relevance=0.7):\n",
        "    # Load the embedding model\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
        "\n",
        "    # Search for relevant chunks\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=top_k)\n",
        "\n",
        "    filtered_results = [doc for doc, score in results if score >= min_relevance]\n",
        "\n",
        "    if not filtered_results:\n",
        "        context_text = \"No relevant context found.\"\n",
        "    else:\n",
        "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in filtered_results])\n",
        "\n",
        "    # Prepare chat messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful cricket analyst AI.\"},\n",
        "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(context=context_text, question=query_text)}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=HF_MODEL,\n",
        "            messages=messages,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        # Extract response safely\n",
        "        response_text = completion.choices[0].message.content if completion.choices else \"‚ùå Error: No response.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        response_text = f\"‚ùå API Error: {str(e)}\"\n",
        "\n",
        "\n",
        "    sources = [doc.metadata.get(\"source\", \"Unknown Source\") for doc in filtered_results]\n",
        "    formatted_response = f\"üìå **Response:**\\n{response_text}\\n\\nüìö **Sources:** {', '.join(set(sources))}\"\n",
        "\n",
        "    print(formatted_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = input(\"üîé Enter your query: \")\n",
        "    query_database(query)\n",
        "\n"
      ],
      "metadata": {
        "id": "N0g2kPDOt8EV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}